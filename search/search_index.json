{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Cluster Documentation","text":"<p>This site provides documentation for both normal users and system administrators.</p> <p>Use the navigation bar on the left to browse available topics. You can contribute updates through the integrated CMS or by editing Markdown files directly.</p> <p>Fonts are loaded to support both English and Thai content. Syntax-highlighted code blocks and a search box make it easy to find what you need. Use the sun/moon icon at the top right of the page to toggle between light and dark themes at any time.</p>"},{"location":"ist-cluster/","title":"IST-CLUSTER Documentation","text":"<p>The following topics are covered in the IST-CLUSTER user guide:</p> <ul> <li>Introduction to IST-CLUSTER</li> <li>Quick Start Guide</li> <li>Getting access to IST-CLUSTER</li> <li>Getting started</li> <li>IST-CLUSTER overview</li> <li>Login to IST-CLUSTER</li> <li>Running your first job</li> <li>Manage your jobs</li> <li>Checking spent SHr in jobs</li> <li>Storage quota</li> <li>File transfer</li> <li>Software environment</li> <li>Software installation guideline</li> <li>Application Software</li> <li>AI, Machine learning, and Deep learning</li> <li>Bioinformatics</li> <li>Chemistry</li> <li>Climate and Weather</li> <li>Compiler and Toolchain</li> <li>Computer-Aided Engineering (CAE)</li> <li>Container (Apptainer/Singularity)</li> <li>Programming language (R, Python, Perl)</li> <li>\u0e01\u0e32\u0e23\u0e02\u0e2d\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e4c\u0e01\u0e32\u0e23\u0e40\u0e02\u0e49\u0e32\u0e16\u0e36\u0e07 software</li> <li>PI e-service</li> <li>Adding New Team Member</li> <li>Requesting for Service Node-Hour (SHr)</li> <li>Requesting for Project Home Storage Expansion</li> <li>Requesting for Extend Project Duration</li> <li>Training</li> <li>Upcoming workshop</li> <li>All training materials</li> <li>Update Notice</li> <li>Recent update (June/2025)</li> <li>ThaiSC support service</li> <li>Send a support ticket to thaisc-support@nstda.or.th</li> <li>Guideline for submitting a ticket</li> <li>Scope of technical support</li> <li>FAQ</li> <li>Incorrect storage quota</li> <li>Job is pending with \"ReqNodeNotAvail, Reserved for maintenance\"</li> <li>Mamba cannot connect to the internet with the error of [Errno 101] Network is unreachable</li> <li>Migrating from Miniconda to Mamba</li> <li>Usage policy</li> <li>IST-CLUSTER Frontend usage policy</li> </ul>"},{"location":"slurm-basics/","title":"Slurm Cluster Basics","text":"<p>This page provides quick links to detailed guides for using the cluster. Follow each link for step-by-step instructions and examples of common commands.</p> <ul> <li>Overview</li> <li>Accessing the Cluster</li> <li>Environment Setup</li> <li>Submitting Batch Jobs</li> <li>Running Interactive Jobs</li> <li>Monitoring and Managing Jobs</li> <li>Storage and File Systems</li> <li>Helpful Commands</li> <li>Getting Help</li> </ul>"},{"location":"scripts/","title":"Cluster Scripts","text":"<p>This section contains scripts useful for cluster administration.</p>"},{"location":"slurm/accessing/","title":"Accessing the Cluster","text":""},{"location":"slurm/accessing/#ssh-login","title":"SSH Login","text":"<p>Use SSH to connect to the login node:</p> <pre><code>ssh &lt;username&gt;@cluster.example.com\n</code></pre> <p>Replace <code>&lt;username&gt;</code> with your account name. The first time you log in, you may be asked to verify the host key. Some institutions require connecting through a VPN or multi-factor authentication, so consult your local IT instructions if you encounter connection issues.</p>"},{"location":"slurm/accessing/#account-creation","title":"Account Creation","text":"<p>Accounts are provisioned by the cluster administrators. After your account is created, log in and change your password with:</p> <pre><code>passwd\n</code></pre> <p>Choose a strong password and never share it with others.</p> <p>For details about connecting with OpenSSH see the official manual.</p>"},{"location":"slurm/batch-jobs/","title":"Submitting Batch Jobs","text":"<p>Prepare a job script and submit it with <code>sbatch</code>.</p>"},{"location":"slurm/batch-jobs/#example-script","title":"Example Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --time=01:00:00\n#SBATCH --mem=4G\n#SBATCH --output=slurm-%j.out\n\n# Request one GPU if needed\n#SBATCH --gres=gpu:1\n\nsrun my_application\n</code></pre> <p>Submit the job:</p> <pre><code>sbatch job.sh\n</code></pre> <p>Check its status with:</p> <pre><code>squeue -u $USER\n</code></pre> <p>See the sbatch documentation for additional directives such as email notifications or array jobs.</p>"},{"location":"slurm/environment/","title":"Environment Setup","text":"<p>The cluster uses environment modules to manage compilers, MPI libraries and application software. Modules allow you to easily switch between different versions of tools without altering your shell configuration.</p>"},{"location":"slurm/environment/#loading-modules","title":"Loading Modules","text":"<p>View available modules with:</p> <pre><code>module avail\n</code></pre> <p>List currently loaded modules with <code>module list</code> and reset your environment using <code>module purge</code> if needed.</p> <p>Load a compiler or application module, for example:</p> <pre><code>module load gcc/12.1.0\nmodule load openmpi\n</code></pre>"},{"location":"slurm/environment/#virtual-environments","title":"Virtual Environments","text":"<p>To create a Python virtual environment for your own software stack:</p> <pre><code>module load python\npython -m venv ~/envs/myenv\nsource ~/envs/myenv/bin/activate\n</code></pre> <p>Install packages as needed in the activated environment.</p> <p>See the Lmod documentation for more module commands and examples.</p>"},{"location":"slurm/help/","title":"Getting Help","text":""},{"location":"slurm/help/#documentation","title":"Documentation","text":"<p>Slurm provides extensive manual pages. For example:</p> <pre><code>man sbatch\n</code></pre> <p>You can view help for most commands with the <code>--help</code> flag, and the official documentation is available online at https://slurm.schedmd.com/documentation.html.</p> <p>Additional guides may be found on the cluster website.</p>"},{"location":"slurm/help/#contact-support","title":"Contact Support","text":"<p>If you encounter problems, email <code>cluster-support@example.com</code> or open a support ticket through the service portal. Include your job ID and any error messages to help staff diagnose the issue.</p>"},{"location":"slurm/helpful-commands/","title":"Helpful Commands","text":""},{"location":"slurm/helpful-commands/#slurm-utilities","title":"Slurm Utilities","text":"<ul> <li><code>sinfo</code> \u2013 view available partitions and nodes</li> <li><code>scontrol show job &lt;jobid&gt;</code> \u2013 detailed job information</li> <li><code>squeue -l</code> \u2013 extended job listing</li> </ul> <p>Use <code>sinfo</code> frequently to check which partitions are available. The official man page provides a full description of its output at slurm.schedmd.com/sinfo.html.</p>"},{"location":"slurm/helpful-commands/#troubleshooting-tips","title":"Troubleshooting Tips","text":"<p>Check your job's output and error files if it fails to run. The command <code>scontrol show job &lt;jobid&gt;</code> often provides clues about resource or configuration issues.</p>"},{"location":"slurm/interactive-jobs/","title":"Running Interactive Jobs","text":"<p>Interactive sessions are useful for debugging or running short tasks.</p>"},{"location":"slurm/interactive-jobs/#basic-usage","title":"Basic Usage","text":"<p>Request an interactive shell with:</p> <pre><code>srun --pty bash\n</code></pre> <p>Specify resources such as the partition, CPUs, or runtime to avoid using defaults that may not suit your workload:</p> <pre><code>srun --partition=general --time=30:00 --cpus-per-task=4 --pty bash\n</code></pre>"},{"location":"slurm/interactive-jobs/#requesting-gpus","title":"Requesting GPUs","text":"<p>To run on a GPU, ask for the resource explicitly:</p> <pre><code>srun --gres=gpu:1 --pty bash\n</code></pre> <p>Add other options such as memory or time limits as needed. More examples are available in the srun documentation.</p>"},{"location":"slurm/job-management/","title":"Monitoring and Managing Jobs","text":""},{"location":"slurm/job-management/#viewing-jobs","title":"Viewing Jobs","text":"<p>See your queued or running jobs with:</p> <pre><code>squeue -u $USER\n</code></pre> <p>Add <code>-p &lt;partition&gt;</code> to filter by partition. Refer to the squeue documentation for more options.</p>"},{"location":"slurm/job-management/#job-history","title":"Job History","text":"<p>After a job finishes, view its history using:</p> <pre><code>sacct -j &lt;jobid&gt;\n</code></pre> <p>Specify a start time with <code>-S</code> to view older records:</p> <pre><code>sacct -S yesterday\n</code></pre>"},{"location":"slurm/job-management/#canceling-jobs","title":"Canceling Jobs","text":"<p>Stop a running or pending job with:</p> <pre><code>scancel &lt;jobid&gt;\n</code></pre> <p>You can cancel all of your jobs with <code>scancel -u $USER</code>. See the scancel documentation for additional usage details.</p>"},{"location":"slurm/overview/","title":"Overview","text":"<p>The cluster provides high-performance computing resources for research and analysis tasks. Typical workloads include simulations, data processing, training machine learning models, and other compute-intensive jobs. Nodes are equipped with modern CPUs and select partitions include GPUs for accelerated workloads.</p>"},{"location":"slurm/overview/#policies-and-usage-agreements","title":"Policies and Usage Agreements","text":"<p>Users must comply with institutional policies and only run authorized workloads. Do not share accounts or run services unrelated to your research. Refer to the HPC usage policy for details. Contact the administrators if you are unsure about any usage restrictions.</p>"},{"location":"slurm/storage/","title":"Storage and File Systems","text":""},{"location":"slurm/storage/#home-and-scratch","title":"Home and Scratch","text":"<p>Your home directory is backed up and should be used for important files and scripts. Scratch space provides larger, high-performance storage for running jobs and temporary data.</p>"},{"location":"slurm/storage/#best-practices","title":"Best Practices","text":"<ul> <li>Run jobs from scratch to avoid filling home quotas.</li> <li>Clean up large files when they are no longer needed.</li> <li>Keep personal backups of critical data.</li> <li>Check your quota with <code>lfs quota /scratch/$USER</code> if available.</li> </ul> <p>More details about the storage layout are available from the cluster storage guide.</p>"}]}