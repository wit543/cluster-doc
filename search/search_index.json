{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Cluster Documentation","text":"<p>This site provides documentation for both normal users and system administrators.</p> <p>Use the navigation bar on the left to browse available topics. You can contribute updates through the integrated CMS or by editing Markdown files directly.</p> <p>Fonts are loaded to support both English and Thai content. Syntax-highlighted code blocks and a search box make it easy to find what you need. Use the sun/moon icon at the top right of the page to toggle between light and dark themes at any time.</p>"},{"location":"apptainer/","title":"Apptainer Containers","text":"<p>Apptainer (formerly Singularity) allows you to run containerized applications without requiring administrator privileges. It is commonly used to package software stacks for HPC clusters.</p>"},{"location":"apptainer/#loading-the-apptainer-module","title":"Loading the Apptainer Module","text":"<p>First load the module provided by the cluster:</p> <pre><code>module load apptainer\n</code></pre>"},{"location":"apptainer/#obtaining-an-image","title":"Obtaining an Image","text":"<p>You can pull a pre-built image from an online registry or build one yourself. To download from Docker Hub:</p> <pre><code>apptainer pull ubuntu.sif docker://ubuntu:22.04\n</code></pre>"},{"location":"apptainer/#running-the-container","title":"Running the Container","text":"<p>Execute commands inside the container with <code>run</code> or <code>exec</code>:</p> <pre><code>apptainer run ubuntu.sif\napptainer exec ubuntu.sif /bin/bash\n</code></pre>"},{"location":"apptainer/#building-your-own-image","title":"Building Your Own Image","text":"<p>If you have a definition file named <code>recipe.def</code> you can build a SIF image:</p> <pre><code>apptainer build my_image.sif recipe.def\n</code></pre> <p>Apptainer will create the container in the current directory. You can then distribute the single SIF file to other systems.</p> <p>See the official Apptainer documentation for more options and configuration details.</p>"},{"location":"ist-cluster/","title":"IST-CLUSTER Documentation","text":"<p>The following topics are covered in the IST-CLUSTER user guide:</p> <ul> <li>Introduction to IST-CLUSTER</li> <li>Quick Start Guide</li> <li>Getting access to IST-CLUSTER</li> <li>Getting started</li> <li>IST-CLUSTER overview</li> <li>Login to IST-CLUSTER</li> <li>Running your first job</li> <li>Manage your jobs</li> <li>Checking spent SHr in jobs</li> <li>Storage quota</li> <li>File transfer</li> <li>Software environment</li> <li>Software installation guideline</li> <li>Application Software</li> <li>AI, Machine learning, and Deep learning</li> <li>Bioinformatics</li> <li>Chemistry</li> <li>Climate and Weather</li> <li>Compiler and Toolchain</li> <li>Computer-Aided Engineering (CAE)</li> <li>Container (Apptainer/Singularity)</li> <li>Programming language (R, Python, Perl)</li> <li>\u0e01\u0e32\u0e23\u0e02\u0e2d\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e4c\u0e01\u0e32\u0e23\u0e40\u0e02\u0e49\u0e32\u0e16\u0e36\u0e07 software</li> <li>PI e-service</li> <li>Adding New Team Member</li> <li>Requesting for Service Node-Hour (SHr)</li> <li>Requesting for Project Home Storage Expansion</li> <li>Requesting for Extend Project Duration</li> <li>Training</li> <li>Upcoming workshop</li> <li>All training materials</li> <li>Update Notice</li> <li>Recent update (June/2025)</li> <li>ThaiSC support service</li> <li>Send a support ticket to thaisc-support@nstda.or.th</li> <li>Guideline for submitting a ticket</li> <li>Scope of technical support</li> <li>FAQ</li> <li>Incorrect storage quota</li> <li>Job is pending with \"ReqNodeNotAvail, Reserved for maintenance\"</li> <li>Mamba cannot connect to the internet with the error of [Errno 101] Network is unreachable</li> <li>Migrating from Miniconda to Mamba</li> <li>Usage policy</li> <li>IST-CLUSTER Frontend usage policy</li> </ul>"},{"location":"linux-basics/","title":"Basic Linux Usage","text":"<p>Linux is an operating system commonly used on servers and clusters. This page outlines a few simple commands to help you get started.</p>"},{"location":"linux-basics/#navigating-the-file-system","title":"Navigating the File System","text":"<p>Check your current location:</p> <pre><code>pwd\n</code></pre> <p>List files and directories:</p> <pre><code>ls -l\n</code></pre> <p>Change directories:</p> <pre><code>cd /path/to/directory\n</code></pre>"},{"location":"linux-basics/#working-with-files","title":"Working with Files","text":"<p>Create an empty file:</p> <pre><code>touch newfile.txt\n</code></pre> <p>Copy a file:</p> <pre><code>cp source.txt destination.txt\n</code></pre> <p>Move or rename a file:</p> <pre><code>mv oldname.txt newname.txt\n</code></pre> <p>Remove a file:</p> <pre><code>rm unwanted.txt\n</code></pre> <p>Create a directory:</p> <pre><code>mkdir myfolder\n</code></pre>"},{"location":"linux-basics/#viewing-file-contents","title":"Viewing File Contents","text":"<p>Quickly print a file to the terminal:</p> <pre><code>cat file.txt\n</code></pre> <p>Scroll through a file one page at a time:</p> <pre><code>less file.txt\n</code></pre> <p>Exit <code>less</code> by pressing <code>q</code>.</p>"},{"location":"linux-basics/#installing-packages","title":"Installing Packages","text":"<p>Debian-based distributions use <code>apt</code> for package management:</p> <pre><code>sudo apt update\nsudo apt install package-name\n</code></pre>"},{"location":"linux-basics/#getting-help","title":"Getting Help","text":"<p>Most commands include manual pages. Use <code>man</code> followed by the command name to read them:</p> <pre><code>man ls\n</code></pre> <p>These basics should help you begin working with Linux on the cluster.</p>"},{"location":"modules/","title":"Using Modules","text":"<p>Environment modules allow you to easily switch between different versions of software on the cluster. The module command is provided by Lmod.</p>"},{"location":"modules/#listing-modules","title":"Listing Modules","text":"<p>View everything that is available:</p> <pre><code>module avail\n</code></pre> <p>If you are looking for a particular package, search by name:</p> <pre><code>module spider &lt;package&gt;\n</code></pre>"},{"location":"modules/#loading-a-module","title":"Loading a Module","text":"<p>Once you know the module name, load it into your environment:</p> <pre><code>module load gcc/12.1.0\n</code></pre> <p>List what is currently active with:</p> <pre><code>module list\n</code></pre> <p>Reset your environment and unload all modules with <code>module purge</code>.</p>"},{"location":"modules/#checking-the-module-system-version","title":"Checking the Module System Version","text":"<p>To see the installed Lmod version run:</p> <pre><code>module --version\n</code></pre> <p>More details and examples can be found in the Environment Setup guide.</p>"},{"location":"singularity/","title":"Singularity Containers","text":"<p>Singularity was the original name of Apptainer. Many clusters still ship the command as <code>singularity</code>.</p>"},{"location":"singularity/#loading-the-singularity-module","title":"Loading the Singularity Module","text":"<p>If available, load it like any other module:</p> <pre><code>module load singularity\n</code></pre>"},{"location":"singularity/#downloading-an-image","title":"Downloading an Image","text":"<p>Images can be pulled from remote registries or local paths. For example:</p> <pre><code>singularity pull alpine.sif docker://alpine:latest\n</code></pre>"},{"location":"singularity/#using-the-image","title":"Using the Image","text":"<p>Run a shell inside the container or execute a specific program:</p> <pre><code>singularity shell alpine.sif\nsingularity exec alpine.sif cat /etc/os-release\n</code></pre>"},{"location":"singularity/#building-custom-images","title":"Building Custom Images","text":"<p>With a definition file you can create your own container:</p> <pre><code>singularity build custom.sif Singularity.def\n</code></pre> <p>Refer to the Singularity documentation for a complete walkthrough of advanced features and configuration options.</p>"},{"location":"slurm-basics/","title":"Slurm Cluster Basics","text":"<p>This page provides quick links to detailed guides for using the cluster. Follow each link for step-by-step instructions and examples of common commands.</p> <ul> <li>Overview</li> <li>Accessing the Cluster</li> <li>Environment Setup</li> <li>Submitting Batch Jobs</li> <li>Running Interactive Jobs</li> <li>Monitoring and Managing Jobs</li> <li>Storage and File Systems</li> <li>Helpful Commands</li> <li>Getting Help</li> </ul>"},{"location":"scripts/","title":"Cluster Scripts","text":"<p>This section contains scripts useful for cluster administration.</p>"},{"location":"slurm/accessing/","title":"Accessing the Cluster","text":""},{"location":"slurm/accessing/#ssh-login","title":"SSH Login","text":"<p>Use SSH to connect to the login node:</p> <pre><code>ssh &lt;username&gt;@cluster.example.com\n</code></pre> <p>Replace <code>&lt;username&gt;</code> with your account name. The first time you log in, you may be asked to verify the host key. Some institutions require connecting through a VPN or multi-factor authentication, so consult your local IT instructions if you encounter connection issues.</p>"},{"location":"slurm/accessing/#account-creation","title":"Account Creation","text":"<p>Accounts are provisioned by the cluster administrators. After your account is created, log in and change your password with:</p> <pre><code>passwd\n</code></pre> <p>Choose a strong password and never share it with others.</p> <p>For details about connecting with OpenSSH see the official manual.</p>"},{"location":"slurm/batch-jobs/","title":"Submitting Batch Jobs","text":"<p>Prepare a job script and submit it with <code>sbatch</code>.</p>"},{"location":"slurm/batch-jobs/#example-script","title":"Example Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --time=01:00:00\n#SBATCH --mem=4G\n#SBATCH --output=slurm-%j.out\n\n# Request one GPU if needed\n#SBATCH --gres=gpu:1\n\nsrun my_application\n</code></pre> <p>Submit the job:</p> <pre><code>sbatch job.sh\n</code></pre> <p>Check its status with:</p> <pre><code>squeue -u $USER\n</code></pre> <p>See the sbatch documentation for additional directives such as email notifications or array jobs.</p>"},{"location":"slurm/environment/","title":"Environment Setup","text":"<p>The cluster uses environment modules to manage compilers, MPI libraries and application software. Modules allow you to easily switch between different versions of tools without altering your shell configuration.</p>"},{"location":"slurm/environment/#loading-modules","title":"Loading Modules","text":"<p>View available modules with:</p> <pre><code>module avail\n</code></pre> <p>List currently loaded modules with <code>module list</code> and reset your environment using <code>module purge</code> if needed.</p> <p>Load a compiler or application module, for example:</p> <pre><code>module load gcc/12.1.0\nmodule load openmpi\n</code></pre>"},{"location":"slurm/environment/#virtual-environments","title":"Virtual Environments","text":"<p>To create a Python virtual environment for your own software stack:</p> <pre><code>module load python\npython -m venv ~/envs/myenv\nsource ~/envs/myenv/bin/activate\n</code></pre> <p>Install packages as needed in the activated environment.</p> <p>See the Lmod documentation for more module commands and examples.</p>"},{"location":"slurm/help/","title":"Getting Help","text":""},{"location":"slurm/help/#documentation","title":"Documentation","text":"<p>Slurm provides extensive manual pages. For example:</p> <pre><code>man sbatch\n</code></pre> <p>You can view help for most commands with the <code>--help</code> flag, and the official documentation is available online at https://slurm.schedmd.com/documentation.html.</p> <p>Additional guides may be found on the cluster website.</p>"},{"location":"slurm/help/#contact-support","title":"Contact Support","text":"<p>If you encounter problems, email <code>cluster-support@example.com</code> or open a support ticket through the service portal. Include your job ID and any error messages to help staff diagnose the issue.</p>"},{"location":"slurm/helpful-commands/","title":"Helpful Commands","text":""},{"location":"slurm/helpful-commands/#slurm-utilities","title":"Slurm Utilities","text":"<ul> <li><code>sinfo</code> \u2013 view available partitions and nodes</li> <li><code>scontrol show job &lt;jobid&gt;</code> \u2013 detailed job information</li> <li><code>squeue -l</code> \u2013 extended job listing</li> </ul> <p>Use <code>sinfo</code> frequently to check which partitions are available. The official man page provides a full description of its output at slurm.schedmd.com/sinfo.html.</p>"},{"location":"slurm/helpful-commands/#troubleshooting-tips","title":"Troubleshooting Tips","text":"<p>Check your job's output and error files if it fails to run. The command <code>scontrol show job &lt;jobid&gt;</code> often provides clues about resource or configuration issues.</p>"},{"location":"slurm/interactive-jobs/","title":"Running Interactive Jobs","text":"<p>Interactive sessions are useful for debugging or running short tasks.</p>"},{"location":"slurm/interactive-jobs/#basic-usage","title":"Basic Usage","text":"<p>Request an interactive shell with:</p> <pre><code>srun --pty bash\n</code></pre> <p>Specify resources such as the partition, CPUs, or runtime to avoid using defaults that may not suit your workload:</p> <pre><code>srun --partition=general --time=30:00 --cpus-per-task=4 --pty bash\n</code></pre>"},{"location":"slurm/interactive-jobs/#requesting-gpus","title":"Requesting GPUs","text":"<p>To run on a GPU, ask for the resource explicitly:</p> <pre><code>srun --gres=gpu:1 --pty bash\n</code></pre> <p>Add other options such as memory or time limits as needed. More examples are available in the srun documentation.</p>"},{"location":"slurm/job-management/","title":"Monitoring and Managing Jobs","text":""},{"location":"slurm/job-management/#viewing-jobs","title":"Viewing Jobs","text":"<p>See your queued or running jobs with:</p> <pre><code>squeue -u $USER\n</code></pre> <p>Add <code>-p &lt;partition&gt;</code> to filter by partition. Refer to the squeue documentation for more options.</p>"},{"location":"slurm/job-management/#job-history","title":"Job History","text":"<p>After a job finishes, view its history using:</p> <pre><code>sacct -j &lt;jobid&gt;\n</code></pre> <p>Specify a start time with <code>-S</code> to view older records:</p> <pre><code>sacct -S yesterday\n</code></pre>"},{"location":"slurm/job-management/#canceling-jobs","title":"Canceling Jobs","text":"<p>Stop a running or pending job with:</p> <pre><code>scancel &lt;jobid&gt;\n</code></pre> <p>You can cancel all of your jobs with <code>scancel -u $USER</code>. See the scancel documentation for additional usage details.</p>"},{"location":"slurm/overview/","title":"Overview","text":"<p>The cluster provides high-performance computing resources for research and analysis tasks. Typical workloads include simulations, data processing, training machine learning models, and other compute-intensive jobs. Nodes are equipped with modern CPUs and select partitions include GPUs for accelerated workloads.</p>"},{"location":"slurm/overview/#policies-and-usage-agreements","title":"Policies and Usage Agreements","text":"<p>Users must comply with institutional policies and only run authorized workloads. Do not share accounts or run services unrelated to your research. Refer to the HPC usage policy for details. Contact the administrators if you are unsure about any usage restrictions.</p>"},{"location":"slurm/storage/","title":"Storage and File Systems","text":""},{"location":"slurm/storage/#home-and-scratch","title":"Home and Scratch","text":"<p>Your home directory is backed up and should be used for important files and scripts. Scratch space provides larger, high-performance storage for running jobs and temporary data.</p>"},{"location":"slurm/storage/#best-practices","title":"Best Practices","text":"<ul> <li>Run jobs from scratch to avoid filling home quotas.</li> <li>Clean up large files when they are no longer needed.</li> <li>Keep personal backups of critical data.</li> <li>Check your quota with <code>lfs quota /scratch/$USER</code> if available.</li> </ul> <p>More details about the storage layout are available from the cluster storage guide.</p>"}]}