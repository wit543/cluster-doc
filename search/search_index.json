{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Cluster Documentation","text":"<p>This site provides documentation for both normal users and system administrators.</p> <p>Use the navigation bar on the left to browse available topics. You can contribute updates through the integrated CMS or by editing Markdown files directly.</p> <p>Fonts are loaded to support both English and Thai content. Syntax-highlighted code blocks and a search box make it easy to find what you need. Use the sun/moon icon at the top right of the page to toggle between light and dark themes at any time.</p>"},{"location":"apptainer/","title":"Apptainer Containers","text":"<p>Apptainer (formerly Singularity) allows you to run containerized applications without requiring administrator privileges. It is commonly used to package software stacks for HPC clusters.</p>"},{"location":"apptainer/#loading-the-apptainer-module","title":"Loading the Apptainer Module","text":"<p>First load the module provided by the cluster:</p> <pre><code>module load apptainer\n</code></pre>"},{"location":"apptainer/#obtaining-an-image","title":"Obtaining an Image","text":"<p>You can pull a pre-built image from an online registry or build one yourself. To download from Docker Hub:</p> <pre><code>apptainer pull ubuntu.sif docker://ubuntu:22.04\n</code></pre>"},{"location":"apptainer/#running-the-container","title":"Running the Container","text":"<p>Execute commands inside the container with <code>run</code> or <code>exec</code>:</p> <pre><code>apptainer run ubuntu.sif\napptainer exec ubuntu.sif /bin/bash\n</code></pre>"},{"location":"apptainer/#building-your-own-image","title":"Building Your Own Image","text":"<p>If you have a definition file named <code>recipe.def</code> you can build a SIF image:</p> <pre><code>apptainer build my_image.sif recipe.def\n</code></pre> <p>Apptainer will create the container in the current directory. You can then distribute the single SIF file to other systems.</p> <p>See the official Apptainer documentation for more options and configuration details.</p>"},{"location":"ist-cluster/","title":"IST-CLUSTER Documentation","text":"<p>The following topics are covered in the IST-CLUSTER user guide:</p> <ul> <li>Introduction to IST-CLUSTER</li> <li>Quick Start Guide</li> <li>Getting access to IST-CLUSTER</li> <li>Getting started</li> <li>IST-CLUSTER overview</li> <li>Login to IST-CLUSTER</li> <li>Running your first job</li> <li>Manage your jobs</li> <li>Checking spent SHr in jobs</li> <li>Storage quota</li> <li>File transfer</li> <li>Software environment</li> <li>Software installation guideline</li> <li>Application Software</li> <li>AI, Machine learning, and Deep learning</li> <li>Bioinformatics</li> <li>Chemistry</li> <li>Climate and Weather</li> <li>Compiler and Toolchain</li> <li>Computer-Aided Engineering (CAE)</li> <li>Container (Apptainer/Singularity)</li> <li>Programming language (R, Python, Perl)</li> <li>\u0e01\u0e32\u0e23\u0e02\u0e2d\u0e2a\u0e34\u0e17\u0e18\u0e34\u0e4c\u0e01\u0e32\u0e23\u0e40\u0e02\u0e49\u0e32\u0e16\u0e36\u0e07 software</li> <li>PI e-service</li> <li>Adding New Team Member</li> <li>Requesting for Service Node-Hour (SHr)</li> <li>Requesting for Project Home Storage Expansion</li> <li>Requesting for Extend Project Duration</li> <li>Training</li> <li>Upcoming workshop</li> <li>All training materials</li> <li>Update Notice</li> <li>Recent update (June/2025)</li> <li>ThaiSC support service</li> <li>Send a support ticket to thaisc-support@nstda.or.th</li> <li>Guideline for submitting a ticket</li> <li>Scope of technical support</li> <li>FAQ</li> <li>Incorrect storage quota</li> <li>Job is pending with \"ReqNodeNotAvail, Reserved for maintenance\"</li> <li>Mamba cannot connect to the internet with the error of [Errno 101] Network is unreachable</li> <li>Migrating from Miniconda to Mamba</li> <li>Usage policy</li> <li>IST-CLUSTER Frontend usage policy</li> </ul>"},{"location":"linux-basics/","title":"Basic Linux Usage","text":"<p>Linux is an operating system commonly used on servers and clusters. This page outlines a few simple commands to help you get started.</p>"},{"location":"linux-basics/#navigating-the-file-system","title":"Navigating the File System","text":"<p>Check your current location:</p> <pre><code>pwd\n</code></pre> <p>List files and directories:</p> <pre><code>ls -l\n</code></pre> <p>Change directories:</p> <pre><code>cd /path/to/directory\n</code></pre>"},{"location":"linux-basics/#working-with-files","title":"Working with Files","text":"<p>Create an empty file:</p> <pre><code>touch newfile.txt\n</code></pre> <p>Copy a file:</p> <pre><code>cp source.txt destination.txt\n</code></pre> <p>Move or rename a file:</p> <pre><code>mv oldname.txt newname.txt\n</code></pre> <p>Remove a file:</p> <pre><code>rm unwanted.txt\n</code></pre> <p>Create a directory:</p> <pre><code>mkdir myfolder\n</code></pre>"},{"location":"linux-basics/#viewing-file-contents","title":"Viewing File Contents","text":"<p>Quickly print a file to the terminal:</p> <pre><code>cat file.txt\n</code></pre> <p>Scroll through a file one page at a time:</p> <pre><code>less file.txt\n</code></pre> <p>Exit <code>less</code> by pressing <code>q</code>.</p>"},{"location":"linux-basics/#installing-packages","title":"Installing Packages","text":"<p>Debian-based distributions use <code>apt</code> for package management:</p> <pre><code>sudo apt update\nsudo apt install package-name\n</code></pre>"},{"location":"linux-basics/#getting-help","title":"Getting Help","text":"<p>Most commands include manual pages. Use <code>man</code> followed by the command name to read them:</p> <pre><code>man ls\n</code></pre> <p>These basics should help you begin working with Linux on the cluster.</p>"},{"location":"modules/","title":"Using Modules","text":"<p>Environment modules allow you to easily switch between different versions of software on the cluster. The module command is provided by Lmod.</p>"},{"location":"modules/#listing-modules","title":"Listing Modules","text":"<p>View everything that is available:</p> <pre><code>module avail\n</code></pre> <p>If you are looking for a particular package, search by name:</p> <pre><code>module spider &lt;package&gt;\n</code></pre>"},{"location":"modules/#loading-a-module","title":"Loading a Module","text":"<p>Once you know the module name, load it into your environment:</p> <pre><code>module load gcc/12.1.0\n</code></pre> <p>List what is currently active with:</p> <pre><code>module list\n</code></pre> <p>Reset your environment and unload all modules with <code>module purge</code>.</p>"},{"location":"modules/#checking-the-module-system-version","title":"Checking the Module System Version","text":"<p>To see the installed Lmod version run:</p> <pre><code>module --version\n</code></pre> <p>More details and examples can be found in the Environment Setup guide.</p>"},{"location":"singularity/","title":"Singularity Containers","text":"<p>Singularity was the original name of Apptainer. Many clusters still ship the command as <code>singularity</code>.</p>"},{"location":"singularity/#loading-the-singularity-module","title":"Loading the Singularity Module","text":"<p>If available, load it like any other module:</p> <pre><code>module load singularity\n</code></pre>"},{"location":"singularity/#downloading-an-image","title":"Downloading an Image","text":"<p>Images can be pulled from remote registries or local paths. For example:</p> <pre><code>singularity pull alpine.sif docker://alpine:latest\n</code></pre>"},{"location":"singularity/#using-the-image","title":"Using the Image","text":"<p>Run a shell inside the container or execute a specific program:</p> <pre><code>singularity shell alpine.sif\nsingularity exec alpine.sif cat /etc/os-release\n</code></pre>"},{"location":"singularity/#building-custom-images","title":"Building Custom Images","text":"<p>With a definition file you can create your own container:</p> <pre><code>singularity build custom.sif Singularity.def\n</code></pre> <p>Refer to the Singularity documentation for a complete walkthrough of advanced features and configuration options.</p>"},{"location":"slurm-basics/","title":"Slurm Cluster Basics","text":"<p>This page provides quick links to detailed guides for using the cluster. Follow each link for step-by-step instructions and examples of common commands.</p> <ul> <li>Overview</li> <li>Accessing the Cluster</li> <li>Environment Setup</li> <li>Mamba/Anaconda Environments</li> <li>Submitting Batch Jobs</li> <li>Running Interactive Jobs</li> <li>Monitoring and Managing Jobs</li> <li>Inspecting Job Output</li> <li>Storage and File Systems</li> <li>Helpful Commands</li> <li>Getting Help</li> </ul>"},{"location":"user-registration/","title":"User Registration and SSH Key Setup","text":"<p>This page describes how administrators register a new user and install the user's SSH key on the cluster.</p>"},{"location":"user-registration/#determine-the-username","title":"Determine the Username","text":"<ol> <li>Start with the user's first name followed by the first letter of the last name. For example, <code>Jane Doe</code> becomes <code>janed</code>.</li> <li>If a username already exists, append a number until you find an unused name: <code>janed2</code>, <code>janed3</code>, and so on.</li> <li>For interns add <code>_inter</code> to the end, e.g. <code>janed_inter</code>. For guests use <code>_guest</code>.</li> </ol>"},{"location":"user-registration/#create-the-account","title":"Create the Account","text":"<p>Run the following commands as root:</p> <pre><code>sudo useradd -m &lt;username&gt;\nsudo passwd &lt;username&gt;\n</code></pre> <p>This creates the home directory and prompts for an initial password.</p>"},{"location":"user-registration/#generate-an-ssh-key","title":"Generate an SSH Key","text":"<p>Ask the new user to create an SSH key pair on their workstation:</p> <pre><code>ssh-keygen -t ed25519 -C \"&lt;email&gt;\"\n</code></pre> <p>The user should send the resulting <code>id_ed25519.pub</code> file to the administrator.</p>"},{"location":"user-registration/#install-the-public-key","title":"Install the Public Key","text":"<p>Switch to the new account and set up the <code>.ssh</code> folder:</p> <pre><code>su - &lt;username&gt;\nmkdir -m 700 ~/.ssh\ncat /path/to/id_ed25519.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 600 ~/.ssh/authorized_keys\n</code></pre> <p>Ensure the directory and file are owned by the user:</p> <pre><code>chown -R &lt;username&gt;:&lt;username&gt; ~/.ssh\n</code></pre> <p>The user can now log in using their private key. Keep the private key secure and never share it with others.</p>"},{"location":"scripts/","title":"Cluster Scripts","text":"<p>This section contains scripts useful for cluster administration.</p>"},{"location":"scripts/sinfo-to-page/","title":"Converting <code>sinfo</code> Output to Markdown","text":"<p>This script reads the text output from the <code>sinfo</code> command and generates a Markdown summary for each partition.</p>"},{"location":"scripts/sinfo-to-page/#usage","title":"Usage","text":"<ol> <li>Save the output of <code>sinfo</code> to a file:    <code>bash    sinfo &gt; sinfo.txt</code></li> <li>Run the conversion script:    <code>bash    python docs/scripts/sinfo_to_markdown.py sinfo.txt &gt; partitions.md</code></li> <li>Open <code>partitions.md</code> in a viewer or add it to the documentation site.</li> </ol>"},{"location":"scripts/sinfo-to-page/#example","title":"Example","text":"<p>Given an input file containing:</p> <pre><code>PARTITION         AVAIL  TIMELIMIT  NODES  STATE NODELIST\ncpu*                 up 7-00:00:00      4   idle ist-compute-1-[001-004]\n</code></pre> <p>the generated Markdown looks like:</p> <pre><code>## Partition `cpu*`\n\n- **Availability:** up\n- **Time limit:** 7-00:00:00\n\n| State | Nodes | Node list |\n|-------|-------|-----------|\n| idle | 4 | ist-compute-1-[001-004] |\n</code></pre>"},{"location":"slurm/accessing/","title":"Accessing the Cluster","text":""},{"location":"slurm/accessing/#ssh-login","title":"SSH Login","text":"<p>Use SSH to connect to the login node:</p> <pre><code>ssh &lt;username&gt;@cluster.example.com\n</code></pre> <p>Replace <code>&lt;username&gt;</code> with your account name. The first time you log in, you may be asked to verify the host key. Some institutions require connecting through a VPN or multi-factor authentication, so consult your local IT instructions if you encounter connection issues.</p>"},{"location":"slurm/accessing/#account-creation","title":"Account Creation","text":"<p>Accounts are provisioned by the cluster administrators. After your account is created, log in and change your password with:</p> <pre><code>passwd\n</code></pre> <p>Choose a strong password and never share it with others.</p> <p>For details about connecting with OpenSSH see the official manual.</p>"},{"location":"slurm/batch-jobs/","title":"Submitting Batch Jobs","text":"<p>Prepare a job script and submit it with <code>sbatch</code>.</p>"},{"location":"slurm/batch-jobs/#example-script","title":"Example Script","text":"<pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --partition=general\n#SBATCH --nodes=1\n#SBATCH --time=01:00:00\n#SBATCH --mem=4G\n#SBATCH --output=slurm-%j.out\n\n# Request one GPU if needed\n#SBATCH --gres=gpu:1\n\nsrun my_application\n</code></pre> <p>Submit the job:</p> <pre><code>sbatch job.sh\n</code></pre> <p>Check its status with:</p> <pre><code>squeue -u $USER\n</code></pre> <p>See the sbatch documentation for additional directives such as email notifications or array jobs.</p>"},{"location":"slurm/environment/","title":"Environment Setup","text":"<p>The cluster uses environment modules to manage compilers, MPI libraries and application software. Modules allow you to easily switch between different versions of tools without altering your shell configuration.</p>"},{"location":"slurm/environment/#loading-modules","title":"Loading Modules","text":"<p>View available modules with:</p> <pre><code>module avail\n</code></pre> <p>List currently loaded modules with <code>module list</code> and reset your environment using <code>module purge</code> if needed.</p> <p>Load a compiler or application module, for example:</p> <pre><code>module load gcc/12.1.0\nmodule load openmpi\n</code></pre>"},{"location":"slurm/environment/#virtual-environments","title":"Virtual Environments","text":"<p>To create a Python virtual environment for your own software stack:</p> <pre><code>module load python\npython -m venv ~/envs/myenv\nsource ~/envs/myenv/bin/activate\n</code></pre> <p>Install packages as needed in the activated environment.</p> <p>For a Conda-based workflow using Mamba or Anaconda, see Mamba/Anaconda Environments.</p> <p>See the Lmod documentation for more module commands and examples.</p>"},{"location":"slurm/help/","title":"Getting Help","text":""},{"location":"slurm/help/#documentation","title":"Documentation","text":"<p>Slurm provides extensive manual pages. For example:</p> <pre><code>man sbatch\n</code></pre> <p>You can view help for most commands with the <code>--help</code> flag, and the official documentation is available online at https://slurm.schedmd.com/documentation.html.</p> <p>Additional guides may be found on the cluster website.</p>"},{"location":"slurm/help/#contact-support","title":"Contact Support","text":"<p>If you encounter problems, email <code>cluster-support@example.com</code> or open a support ticket through the service portal. Include your job ID and any error messages to help staff diagnose the issue.</p>"},{"location":"slurm/helpful-commands/","title":"Helpful Commands","text":""},{"location":"slurm/helpful-commands/#slurm-utilities","title":"Slurm Utilities","text":"<ul> <li><code>sinfo</code> \u2013 view available partitions and nodes</li> <li><code>scontrol show job &lt;jobid&gt;</code> \u2013 detailed job information</li> <li><code>squeue -l</code> \u2013 extended job listing</li> </ul> <p>Use <code>sinfo</code> frequently to check which partitions are available. The official man page provides a full description of its output at slurm.schedmd.com/sinfo.html.</p>"},{"location":"slurm/helpful-commands/#troubleshooting-tips","title":"Troubleshooting Tips","text":"<p>Check your job's output and error files if it fails to run. The command <code>scontrol show job &lt;jobid&gt;</code> often provides clues about resource or configuration issues.</p>"},{"location":"slurm/interactive-jobs/","title":"Running Interactive Jobs","text":"<p>Interactive sessions are useful for debugging or running short tasks.</p>"},{"location":"slurm/interactive-jobs/#basic-usage","title":"Basic Usage","text":"<p>Request an interactive shell with:</p> <pre><code>srun --pty bash\n</code></pre> <p>Specify resources such as the partition, CPUs, or runtime to avoid using defaults that may not suit your workload:</p> <pre><code>srun --partition=general --time=30:00 --cpus-per-task=4 --pty bash\n</code></pre>"},{"location":"slurm/interactive-jobs/#requesting-gpus","title":"Requesting GPUs","text":"<p>To run on a GPU, ask for the resource explicitly:</p> <pre><code>srun --gres=gpu:1 --pty bash\n</code></pre> <p>Add other options such as memory or time limits as needed. More examples are available in the srun documentation.</p>"},{"location":"slurm/interactive-sessions/","title":"Interactive Sessions","text":"<p>Interactive sessions let you run commands on a compute node and see the output immediately. They are best for debugging or short exploratory tasks.</p>"},{"location":"slurm/interactive-sessions/#when-to-use-an-interactive-session","title":"When to Use an Interactive Session","text":"<ul> <li>Testing and debugging job scripts before submitting them as batch jobs</li> <li>Running quick commands that require immediate feedback</li> <li>Launching interactive tools such as Jupyter notebooks or GUI applications</li> <li>Exploring the environment or installing software interactively</li> </ul> <p>For long-running workloads that do not require live interaction, submit a batch job instead of keeping an interactive session open.</p>"},{"location":"slurm/interactive-sessions/#starting-an-interactive-session","title":"Starting an Interactive Session","text":"<p>Request a shell on a compute node with <code>srun</code>:</p> <pre><code>srun --pty bash\n</code></pre> <p>Specify resources to match your needs:</p> <pre><code>srun --partition=general --time=30:00 --cpus-per-task=4 --mem=4G --pty bash\n</code></pre> <p>To work on a GPU node, include the <code>--gres</code> option:</p> <pre><code>srun --gres=gpu:1 --pty bash\n</code></pre>"},{"location":"slurm/interactive-sessions/#ending-the-session","title":"Ending the Session","text":"<p>When finished, exit the shell to release the resources:</p> <pre><code>exit\n</code></pre>"},{"location":"slurm/job-inspection/","title":"Inspecting Job Output and Status","text":"<p>When a Slurm job does not behave as expected, you can investigate it in several ways. Start with the log files produced by your job script and then consult Slurm utilities for more details.</p>"},{"location":"slurm/job-inspection/#output-and-error-files","title":"Output and Error Files","text":"<p>By default <code>sbatch</code> writes both standard output and standard error to <code>slurm-&lt;jobid&gt;.out</code>. You can change this in your job script using <code>#SBATCH --output</code> and <code>#SBATCH --error</code> directives:</p> <pre><code>#SBATCH --output=myjob.out\n#SBATCH --error=myjob.err\n</code></pre> <p>Check these files first when something goes wrong. They contain all messages printed by your job and many common application errors.</p>"},{"location":"slurm/job-inspection/#scontrol-for-active-jobs","title":"<code>scontrol</code> for Active Jobs","text":"<p>While a job is queued or running, <code>scontrol show job &lt;jobid&gt;</code> displays detailed information such as allocated nodes, memory, and current state. This is useful for confirming that your requested resources were granted.</p> <p>Pros - Real-time view of the job as it runs - Shows reasons why a job is pending</p> <p>Cons - Information disappears after the job record is purged</p>"},{"location":"slurm/job-inspection/#sacct-for-finished-jobs","title":"<code>sacct</code> for Finished Jobs","text":"<p>After completion, use <code>sacct -j &lt;jobid&gt;</code> to view accounting data including elapsed time, memory usage, and exit code.</p> <p>Pros - Summarizes resource usage and final state - Works even after the job has finished</p> <p>Cons - Limited to stored accounting fields; does not show live details</p>"},{"location":"slurm/job-inspection/#sacctmgr-for-account-managers","title":"<code>sacctmgr</code> for Account Managers","text":"<p>Administrators can query the accounting database directly with <code>sacctmgr</code>. Typical users may not have permission, but the command can reveal project and QOS settings when enabled:</p> <pre><code>sacctmgr show job &lt;jobid&gt;\n</code></pre> <p>Pros - Provides authoritative records from the accounting database - Useful for auditing and troubleshooting complex issues</p> <p>Cons - Often restricted to admin roles - More verbose and slower than <code>sacct</code></p>"},{"location":"slurm/job-inspection/#when-to-use-each-tool","title":"When to Use Each Tool","text":"<ol> <li>Output/error files \u2013 first place to look for program messages and errors.</li> <li><code>scontrol</code> \u2013 check a running or pending job's configuration and allocation.</li> <li><code>sacct</code> \u2013 review statistics and exit status after the job ends.</li> <li><code>sacctmgr</code> \u2013 administrative view for historical or policy-related data.</li> </ol> <p>Use them together to diagnose problems: start with the logs, inspect the active job with <code>scontrol</code>, and consult <code>sacct</code> or <code>sacctmgr</code> for post-run details.</p>"},{"location":"slurm/job-management/","title":"Monitoring and Managing Jobs","text":""},{"location":"slurm/job-management/#viewing-jobs","title":"Viewing Jobs","text":"<p>See your queued or running jobs with either <code>squeue -u $USER</code> or the shorthand <code>squeue --me</code>:</p> <pre><code>squeue --me\n</code></pre> <p>Add <code>-p &lt;partition&gt;</code> to filter by partition. Refer to the squeue documentation for more options.</p> <p>The output lists job identifiers, partitions, job names, states, and more:</p> <pre><code> JOBID PARTITION     NAME     USER ST  TIME  NODES NODELIST(REASON)\n 1234  general   myjob    alice  R  0:10      1   node01\n 1235  general   myjob    alice  PD  0:00      1   (Priority)\n</code></pre> <p>The important columns are:</p> <ul> <li>JOBID \u2013 the numeric identifier used with other commands</li> <li>ST \u2013 the state of the job (<code>R</code> running, <code>PD</code> pending, <code>CG</code> completing, <code>CD</code> completed, <code>F</code> failed)</li> <li>NODELIST(REASON) \u2013 either the node running the job or why it is waiting</li> </ul> <p>See the Slurm job states documentation for the full list of codes.</p>"},{"location":"slurm/job-management/#job-history","title":"Job History","text":"<p>After a job finishes, view its history using:</p> <pre><code>sacct -j &lt;jobid&gt;\n</code></pre> <p>Specify a start time with <code>-S</code> to view older records:</p> <pre><code>sacct -S yesterday\n</code></pre>"},{"location":"slurm/job-management/#canceling-jobs","title":"Canceling Jobs","text":"<p>Stop a running or pending job with:</p> <pre><code>scancel &lt;jobid&gt;\n</code></pre> <p>You can cancel all of your jobs with <code>scancel -u $USER</code>. See the scancel documentation for additional usage details.</p>"},{"location":"slurm/job-management/#job-priority","title":"Job Priority","text":"<p>The scheduler decides which jobs start first based on priority. Display the priority of your queued jobs with:</p> <pre><code>squeue --me -o \"%i %9Q %t %M %j\"\n</code></pre> <p><code>%Q</code> shows the numeric priority assigned to each job. Higher values run sooner. For a more detailed breakdown of how priority is calculated, use <code>sprio</code> and consult the Slurm scheduling documentation.</p> <p>Check a job's priority with:</p> <pre><code>sprio -j &lt;jobid&gt;\n</code></pre> <p>For more detail, run <code>scontrol show job &lt;jobid&gt;</code> and look for the <code>Priority</code> field. Slurm calculates priority from factors such as fairshare, job age, and QOS. See the priority overview on the Slurm website for an explanation of how it works.</p>"},{"location":"slurm/mamba-anaconda/","title":"Mamba/Anaconda Environments","text":"<p>Mamba is a drop-in replacement for the <code>conda</code> package manager that resolves dependencies faster. Both tools can create isolated Python environments. Load the provided module and create a new environment as follows:</p> <pre><code>module load mamba  # or module load anaconda\nmamba create -y -n myenv python=3.10\nconda activate myenv\n</code></pre> <p>Install packages inside the activated environment:</p> <pre><code>mamba install numpy pandas\n</code></pre> <p>To use the environment in a Slurm job script, load the module and activate the environment before running your program:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=conda-test\n#SBATCH --partition=general\n#SBATCH --time=00:10:00\n\nmodule load mamba\nconda activate myenv\nsrun python script.py\n</code></pre> <p>Export the environment specification if you need to recreate it later:</p> <pre><code>conda env export &gt; myenv.yml\n</code></pre>"},{"location":"slurm/mpi-multinode/","title":"MPI and Multi-node Jobs","text":"<p>Many parallel applications use the Message Passing Interface (MPI) to communicate across multiple tasks or nodes. The cluster provides OpenMPI modules so you can run distributed workloads through Slurm.</p>"},{"location":"slurm/mpi-multinode/#loading-an-mpi-module","title":"Loading an MPI Module","text":"<p>Load a compiler and MPI library before compiling or running your programs:</p> <pre><code>module load gcc/12.1.0\nmodule load openmpi\n</code></pre>"},{"location":"slurm/mpi-multinode/#example-batch-script","title":"Example Batch Script","text":"<p>The following script requests two nodes with four MPI tasks per node and runs <code>mpi_hello</code> using <code>srun</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=mpi_test\n#SBATCH --partition=general\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --time=00:10:00\n#SBATCH --output=slurm-%j.out\n\nsrun ./mpi_hello\n</code></pre> <p>Submit the script with <code>sbatch mpi_job.sh</code>. Slurm allocates two nodes and launches a total of eight tasks.</p>"},{"location":"slurm/mpi-multinode/#running-mpirun","title":"Running mpirun","text":"<p>You can also use <code>mpirun</code> provided by OpenMPI. Slurm automatically provides the host list so a typical command inside the job script is:</p> <pre><code>mpirun ./mpi_hello\n</code></pre> <p>For hybrid MPI and OpenMP programs, specify the number of MPI tasks and threads. For example, to run four tasks per node with two threads each:</p> <pre><code>#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=2\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nsrun ./hybrid_app\n</code></pre> <p>This configuration launches eight MPI tasks in total with two OpenMP threads per task across the allocated nodes.</p>"},{"location":"slurm/overview/","title":"Overview","text":"<p>The cluster provides high-performance computing resources for research and analysis tasks. Typical workloads include simulations, data processing, training machine learning models, and other compute-intensive jobs. Nodes are equipped with modern CPUs and select partitions include GPUs for accelerated workloads.</p>"},{"location":"slurm/overview/#policies-and-usage-agreements","title":"Policies and Usage Agreements","text":"<p>Users must comply with institutional policies and only run authorized workloads. Do not share accounts or run services unrelated to your research. Refer to the HPC usage policy for details. Contact the administrators if you are unsure about any usage restrictions.</p>"},{"location":"slurm/preemption/","title":"Preemption","text":"<p>Preemption allows high priority jobs to run by pausing or terminating running low priority jobs. It is typically enabled on a dedicated partition or for jobs marked as preemptible.</p>"},{"location":"slurm/preemption/#why-preempt-jobs","title":"Why Preempt Jobs?","text":"<ul> <li>Ensure urgent computations start quickly when resources are scarce.</li> <li>Keep hardware free for high priority workloads.</li> <li>Allow opportunistic jobs to use idle resources without delaying critical work.</li> </ul>"},{"location":"slurm/preemption/#how-preemption-works","title":"How Preemption Works","text":"<p>When a high priority job is scheduled and resources are unavailable, Slurm may preempt running jobs with lower priority. Depending on cluster configuration:</p> <ul> <li>The job may be requeued and restarted later when resources free up.</li> <li>The job may be canceled entirely.</li> </ul> <p>Preempted jobs that are requeued will start from the beginning unless your application supports checkpointing.</p>"},{"location":"slurm/preemption/#submitting-preemptible-jobs","title":"Submitting Preemptible Jobs","text":"<p>Check available partitions with <code>sinfo</code>. Many clusters provide a partition named <code>preempt</code> (or similar) for jobs that can be interrupted.</p> <pre><code>#SBATCH --partition=preempt\n#SBATCH --requeue       # allow Slurm to requeue the job when preempted\n</code></pre> <p>Submit your script with <code>sbatch job.sh</code>. Use <code>squeue</code> to monitor status. Jobs in the preemptible partition may run immediately but can be stopped when resources are required elsewhere.</p>"},{"location":"slurm/preemption/#effect-on-your-job","title":"Effect on Your Job","text":"<ul> <li>Execution may stop at any time if a higher priority job needs the resources.</li> <li>If <code>--requeue</code> was set and the partition allows it, the job is placed back in the queue and will restart from the beginning once resources are free.</li> <li>If not requeued, the job will be canceled and you must resubmit it manually.</li> </ul>"},{"location":"slurm/preemption/#good-use-cases","title":"Good Use Cases","text":"<p>Preemptible partitions are ideal for:</p> <ul> <li>Long, low-priority simulations that do not need to finish quickly.</li> <li>Opportunistic data analysis or testing that can be restarted.</li> <li>Workloads with built-in checkpointing or frequent save points.</li> </ul> <p>Using preemptible resources lets you take advantage of otherwise idle hardware while ensuring that critical jobs are never delayed.</p>"},{"location":"slurm/qos-policy/","title":"QOS Policy","text":"<p>Slurm uses Quality of Service (QOS) rules to manage job priorities and resource limits. The table below shows the current settings as reported by <code>sacctmgr list qos</code>.</p> <p>To update this page, paste the latest command output into <code>qos-policy-data.txt</code> in the same folder.</p> <pre>Loading...</pre>"},{"location":"slurm/storage/","title":"Storage and File Systems","text":""},{"location":"slurm/storage/#home-and-scratch","title":"Home and Scratch","text":"<p>Your home directory is backed up and should be used for important files and scripts. Scratch space provides larger, high-performance storage for running jobs and temporary data.</p>"},{"location":"slurm/storage/#best-practices","title":"Best Practices","text":"<ul> <li>Run jobs from scratch to avoid filling home quotas.</li> <li>Clean up large files when they are no longer needed.</li> <li>Keep personal backups of critical data.</li> <li>Check your quota with <code>lfs quota /scratch/$USER</code> if available.</li> </ul> <p>More details about the storage layout are available from the cluster storage guide.</p>"},{"location":"tools/editors-and-tmux/","title":"Text Editors and Tmux","text":"<p>This guide introduces three common terminal tools used on most Linux clusters: nano, vim, and tmux. Each section starts with the basics and moves on to more advanced usage.</p>"},{"location":"tools/editors-and-tmux/#nano","title":"Nano","text":""},{"location":"tools/editors-and-tmux/#getting-started","title":"Getting Started","text":"<ul> <li>Launch nano with <code>nano filename</code></li> <li>The shortcuts are listed at the bottom of the screen. Use <code>Ctrl+X</code> to exit, <code>Ctrl+O</code> to save.</li> </ul>"},{"location":"tools/editors-and-tmux/#useful-features","title":"Useful Features","text":"<ul> <li><code>Ctrl+W</code> to search within the file</li> <li><code>Alt+\\</code> to jump to the beginning and <code>Alt+/</code> to jump to the end</li> <li>Enable syntax highlighting with <code>nano -Y &lt;language&gt;</code> if available</li> </ul>"},{"location":"tools/editors-and-tmux/#example-scenario-quick-config-edit","title":"Example Scenario: Quick Config Edit","text":"<p>Nano is ideal for small, quick edits when you are unfamiliar with other editors. Run:</p> <pre><code>nano ~/.bashrc\n</code></pre> <p>Edit the file, press <code>Ctrl+O</code> to write out, then <code>Ctrl+X</code> to quit.</p>"},{"location":"tools/editors-and-tmux/#vim","title":"Vim","text":""},{"location":"tools/editors-and-tmux/#basic-commands","title":"Basic Commands","text":"<ul> <li><code>i</code> enters insert mode so you can type</li> <li><code>Esc</code> returns to normal mode</li> <li><code>:w</code> saves the file, <code>:q</code> quits, and <code>:wq</code> saves then quits</li> </ul>"},{"location":"tools/editors-and-tmux/#moving-around","title":"Moving Around","text":"<ul> <li>Use arrow keys or <code>h j k l</code> to move the cursor</li> <li><code>gg</code> jumps to the top, <code>G</code> jumps to the bottom</li> <li><code>/pattern</code> searches forward, <code>n</code> repeats the search</li> </ul>"},{"location":"tools/editors-and-tmux/#intermediate-usage","title":"Intermediate Usage","text":"<ul> <li>Copy (<code>y</code>) and paste (<code>p</code>) lines or selections</li> <li>Undo with <code>u</code>, redo with <code>Ctrl+r</code></li> <li>Split the window with <code>:split</code> or <code>:vsplit</code></li> </ul>"},{"location":"tools/editors-and-tmux/#advanced-techniques","title":"Advanced Techniques","text":"<ul> <li>Record macros with <code>q</code> followed by a register, replay with <code>@</code></li> <li>Use visual block mode with <code>Ctrl+v</code> for column edits</li> <li>Combine commands like <code>:%s/old/new/g</code> for substitutions</li> </ul>"},{"location":"tools/editors-and-tmux/#example-scenario-editing-job-scripts","title":"Example Scenario: Editing Job Scripts","text":"<p>When writing Slurm scripts, vim's syntax highlighting and powerful search make it easy to modify multiple lines. Open a script with:</p> <pre><code>vim submit.sh\n</code></pre> <p>Use search and replace to update module names quickly:</p> <pre><code>:%s/moduleA/moduleB/g\n</code></pre>"},{"location":"tools/editors-and-tmux/#tmux","title":"Tmux","text":""},{"location":"tools/editors-and-tmux/#starting-out","title":"Starting Out","text":"<ul> <li>Launch a new session with <code>tmux</code></li> <li>Detach from the session using <code>Ctrl+b</code> then <code>d</code></li> <li>Reattach with <code>tmux attach</code></li> </ul>"},{"location":"tools/editors-and-tmux/#managing-windows-and-panes","title":"Managing Windows and Panes","text":"<ul> <li>Create a new window with <code>Ctrl+b</code> then <code>c</code></li> <li>Split horizontally with <code>Ctrl+b</code> then <code>\"</code></li> <li>Split vertically with <code>Ctrl+b</code> then <code>%</code></li> <li>Switch panes using <code>Ctrl+b</code> then arrow keys</li> </ul>"},{"location":"tools/editors-and-tmux/#session-management","title":"Session Management","text":"<ul> <li>List sessions with <code>tmux ls</code></li> <li>Create named sessions: <code>tmux new -s mysession</code></li> <li>Attach to a named session: <code>tmux attach -t mysession</code></li> </ul>"},{"location":"tools/editors-and-tmux/#example-scenario-long-running-jobs","title":"Example Scenario: Long-Running Jobs","text":"<p>Use tmux to keep an interactive session alive while monitoring a job:</p> <pre><code>tmux new -s monitor\nwatch -n 30 squeue -u $USER\n</code></pre> <p>Detach with <code>Ctrl+b</code> then <code>d</code> and reattach later with <code>tmux attach -t monitor</code>.</p>"},{"location":"tools/editors-and-tmux/#summary","title":"Summary","text":"<p>Nano is great for quick edits, vim offers powerful editing capabilities, and tmux lets you persist terminal sessions. Learning these tools will significantly improve your efficiency on the cluster.</p>"},{"location":"tools/file-transfer/","title":"File Transfer with <code>rsync</code>, <code>scp</code>, and <code>rclone</code>","text":"<p>Transferring files to and from the cluster can be done in several ways. This page covers three common tools and the most useful flags for each.</p>"},{"location":"tools/file-transfer/#rsync","title":"<code>rsync</code>","text":"<p><code>rsync</code> efficiently synchronizes files and directories between locations. The basic syntax is:</p> <pre><code>rsync [options] SOURCE DESTINATION\n</code></pre> <p>Commonly used flags:</p> <ul> <li><code>-a</code> \u2013 archive mode; copies directories recursively and preserves permissions, symlinks, and timestamps.</li> <li><code>-v</code> \u2013 verbose output showing which files are transferred.</li> <li><code>-z</code> \u2013 compress data during the transfer, useful over slow connections.</li> <li><code>-h</code> \u2013 output numbers in a human-readable format.</li> <li><code>--progress</code> \u2013 display progress during transfer.</li> <li><code>--delete</code> \u2013 delete files in the destination that no longer exist in the source.</li> </ul> <p>Example: synchronize a local directory to a remote cluster home directory.</p> <pre><code>rsync -avz mydata/ username@cluster.example.com:/home/username/mydata/\n</code></pre>"},{"location":"tools/file-transfer/#scp","title":"<code>scp</code>","text":"<p><code>scp</code> copies files over SSH. It works well for one-off transfers of a few files.</p> <pre><code>scp [options] SOURCE DESTINATION\n</code></pre> <p>Useful flags:</p> <ul> <li><code>-r</code> \u2013 recursively copy directories.</li> <li><code>-P</code> \u2013 specify a remote SSH port if it is not the default (22).</li> <li><code>-C</code> \u2013 enable compression.</li> <li><code>-p</code> \u2013 preserve modification times and file modes.</li> <li><code>-v</code> \u2013 verbose output for debugging connection issues.</li> </ul> <p>Example: copy a local file to the remote <code>/scratch</code> directory.</p> <pre><code>scp -C myfile.txt username@cluster.example.com:/scratch/username/\n</code></pre>"},{"location":"tools/file-transfer/#rclone","title":"<code>rclone</code>","text":"<p><code>rclone</code> syncs files to and from many cloud storage providers and remote systems. Install it on your workstation first.</p> <pre><code>rclone [command] [options] SOURCE DESTINATION\n</code></pre> <p>Important flags and commands:</p> <ul> <li><code>config</code> \u2013 interactively set up or edit a remote configuration.</li> <li><code>copy</code> \u2013 copy files from source to destination, skipping identical files.</li> <li><code>sync</code> \u2013 make destination identical to source by copying new files and deleting missing ones.</li> <li><code>-P</code> or <code>--progress</code> \u2013 show progress during transfers.</li> <li><code>--dry-run</code> \u2013 perform a trial run with no changes made; use to verify commands.</li> <li><code>--bwlimit RATE</code> \u2013 limit bandwidth usage (e.g., <code>--bwlimit 10M</code>).</li> </ul> <p>Example: sync a directory from the cluster to Google Drive.</p> <pre><code>rclone sync /home/username/data gdrive:cluster-backup -P\n</code></pre> <p>These tools cover most file transfer needs. <code>rsync</code> is ideal for incremental backups, <code>scp</code> is quick for small copies, and <code>rclone</code> connects to numerous cloud services.</p>"},{"location":"tools/ssh-tunneling/","title":"Accessing Web Applications and X11","text":"<p>Running graphical applications or web interfaces on a compute node often requires forwarding network connections through the login node. SSH tunnels make this possible without opening additional firewall ports.</p>"},{"location":"tools/ssh-tunneling/#forwarding-web-applications","title":"Forwarding Web Applications","text":"<ol> <li>Start an interactive job and note the compute node name:    <code>bash    salloc --partition=general --time=2:00:00 --nodes=1 --cpus-per-task=2</code>    After the job begins you will land on a node such as <code>compute001</code>.</li> <li>Launch your application on that node. For Jupyter Lab:    <code>bash    jupyter lab --no-browser --ip=0.0.0.0 --port=8888</code></li> <li>From your local machine open a second terminal and create an SSH tunnel    through the login node, replacing the host name with the one from step 1:    <code>bash    ssh -L 8888:compute001:8888 &lt;user&gt;@login.cluster.example.com</code>    Point your web browser to <code>http://localhost:8888</code>. Use a different local port    if 8888 is already in use.</li> </ol>"},{"location":"tools/ssh-tunneling/#x11-forwarding","title":"X11 Forwarding","text":"<p>X11 lets you run graphical programs on the cluster while displaying them on your own computer. You need an X server installed locally (XQuartz on macOS, VcXsrv or Xming on Windows, or just X11 on Linux).</p> <ol> <li>Connect to the compute node with X forwarding enabled and jumping through the    login node:    <code>bash    ssh -X -J &lt;user&gt;@login.cluster.example.com &lt;user&gt;@compute001</code></li> <li>Start a program like <code>xterm</code> or <code>xclock</code>. The window should appear on your    desktop.</li> </ol>"},{"location":"tools/ssh-tunneling/#reverse-tunnels","title":"Reverse Tunnels","text":"<p>When direct forwarding from the login node to the compute node is not permitted, you can create a reverse tunnel from the compute node back to the login node:</p> <pre><code>ssh -R 8888:localhost:8888 &lt;user&gt;@login.cluster.example.com\n</code></pre> <p>After this command, connect from your local machine to the login node as follows:</p> <pre><code>ssh -L 8888:localhost:8888 &lt;user&gt;@login.cluster.example.com\n</code></pre> <p>This sends traffic from your web browser through the login node to the compute node running the application.</p>"}]}